# 

# Architecture Cible
```mermaid
flowchart TB

  %% === ORCHESTRATEUR PRINCIPAL ===
  subgraph MainOrchestrator["ORCHESTRATEUR PRINCIPAL - main_orchestrator.py"]
    MainOrch[Main Orchestrator - Apache Airflow]
  end

  %% === PIPELINES ===
  subgraph Pipelines["Pipelines de Collecte"]
    PipeAPI[Pipeline API]
    PipeFile[Pipeline Fichier]
    PipeWeb[Pipeline Web Scraping]
    PipeDB[Pipeline Base de DonnÃ©es SQL - No SQL]
  end

  %% === REDIS QUEUES ===
  subgraph RedisQ["Redis Queues par Source"]
    QueueAPI[Redis Queue - API]
    QueueFile[Redis Queue - Fichier]
    QueueWeb[Redis Queue - Scraping]
    QueueDB[Redis Queue - BD]
  end

  %% === MESSAGE BROKER ===
  subgraph Broker["Message Broker - Redis"]
    BrokerCore[Gestion des files - Communication inter-pipelines]
  end

  %% === WORKERS DE NORMALISATION ===
  subgraph Normalizers["Workers de Normalisation"]
    WorkerAPI[Worker API]
    WorkerFile[Worker Fichiers]
    WorkerWeb[Worker Web Scraping]
    WorkerDB[Worker BD]
  end

  %% === SPARK CORE ===
  subgraph SparkCore["Apache Spark Cluster"]
    SparkSession[Spark Session]
    SparkSQL[Spark SQL - DataFrames]
    SparkStreaming[Spark Streaming]
    SparkML[MLlib - Machine Learning]
  end

  %% === WORKERS D'AGREGATION ===
  subgraph Aggregators["Workers d'AgrÃ©gation Spark"]
    AggAPI[Spark Aggregator - Traitement DistribuÃ©]
  end

  %% === STOCKAGE SPARK ===
  subgraph SparkStorage["Stockage Spark & Data Lake"]
    DataLake[(Data Lake - Parquet/Delta)]
    SparkWarehouse[(Spark Warehouse)]
  end

  %% === BASES FINALES ===
  subgraph FinalDBs["Bases Finales OptimisÃ©es"]
    AnalyticsDB[(PostgreSQL - Analytics)]
    ServingDB[(API Serving Layer)]
  end

  %% === LIENS ===
  MainOrch --> PipeAPI
  MainOrch --> PipeFile
  MainOrch --> PipeWeb
  MainOrch --> PipeDB

  PipeAPI --> QueueAPI
  PipeFile --> QueueFile
  PipeWeb --> QueueWeb
  PipeDB --> QueueDB

  QueueAPI --> BrokerCore
  QueueFile --> BrokerCore
  QueueWeb --> BrokerCore
  QueueDB --> BrokerCore

  BrokerCore --> WorkerAPI
  BrokerCore --> WorkerFile
  BrokerCore --> WorkerWeb
  BrokerCore --> WorkerDB

  WorkerAPI --> SparkSession
  WorkerFile --> SparkSession
  WorkerWeb --> SparkSession
  WorkerDB --> SparkSession

  SparkSession --> AggAPI
  AggAPI --> SparkSQL
  AggAPI --> SparkStreaming
  AggAPI --> SparkML
  
  SparkSQL --> DataLake
  SparkStreaming --> DataLake
  SparkML --> DataLake
  
  DataLake --> SparkWarehouse
  SparkWarehouse --> AnalyticsDB
  SparkWarehouse --> ServingDB
```
## Architecture optimisÃ©ee avec Redis+Spark

- Sources â†’ Redis (buffer/cache) â†’ Spark (traitement) â†’ Data Lake

```mermaid
flowchart TB
    subgraph DataSources[Sources de DonnÃ©es]
        API[API]
        Files[Fichiers]
        Web[Web Scraping]
    end

    subgraph RedisLayer[Redis - Couche Temps RÃ©Ã«l]
        Streams[Redis Streams<br/>Buffer Temps RÃ©Ã«l]
        Cache[Redis Cache<br/>Sessions/Configs]
        Dedup[Bloom Filters<br/>DÃ©duplication]
    end

    subgraph SparkCore[Spark - Traitement Lourd]
        SparkStream[Spark Streaming]
        SparkSQL[Spark SQL]
        SparkML[MLlib]
    end

    subgraph Storage[Stockage Persistant]
        DataLake[Data Lake Parquet]
        Warehouse[Data Warehouse]
    end

    DataSources --> Streams
    Streams --> SparkStream
    Cache --> SparkSQL
    Dedup --> SparkStream
    
    SparkStream --> DataLake
    SparkSQL --> Warehouse
    SparkML --> DataLake

```


```text
Redis
    âœ… Buffer temps rÃ©el entre collecte et Spark
    âœ… Cache des mÃ©tadonnÃ©es et configurations
    âœ… DÃ©duplication en temps rÃ©el
    âœ… Gestion des sessions utilisateur

Spark

    âœ… Stockage temporaire des gros volumes
    âœ… AgrÃ©gations et transformations complexes
    âœ… Machine Learning et analytics
    âœ… Data Lake et entrepÃ´t de donnÃ©e

```
## Architecture Optimisee pour imo-ops
```text
ðŸ“Š Flux de DonnÃ©es OptimisÃ©

    - Collecte â†’ DonnÃ©es vers Redis Streams + Cache metadata
    - Streaming â†’ Spark lit Redis Streams, traite et Ã©crit dans Bronze
    - Batch â†’ Spark nettoie Bronze â†’ Silver â†’ Gold
    - Serving â†’ DonnÃ©es agrÃ©gÃ©es vers PostgreSQL + API

ðŸŽ¯ Avantages de cette Architecture

    âœ… Faible latence avec Redis pour le temps rÃ©el
    âœ… ScalabilitÃ© horizontale avec Spark
    âœ… Data Lake pour l'historique complet
    âœ… DÃ©duplication efficace avec Bloom Filters
    âœ… Monitoring via Airflow
    âœ… SÃ©paration des concerns claire
```
```mermaid
flowchart TB
    %% === ORCHESTRATION ===
    subgraph Orchestration["Orchestration - Apache Airflow"]
        Airflow[Airflow DAGs<br/>Orchestration Globale]
    end

    %% === COLLECTE ===
    subgraph DataCollection["Collecte des DonnÃ©es"]
        API[API Collectors]
        Files[File Collectors]
        Web[Web Scraping]
        DB[Database Collectors]
    end

    %% === REDIS OPTIMISÃ‰ ===
    subgraph RedisLayer["Redis - Couche Temps RÃ©el & Cache"]
        Streams[Redis Streams<br/>Buffer Temps RÃ©el]
        Cache[Redis Cache<br/>Configs/MÃ©tadonnÃ©es]
        Dedup[Bloom Filters<br/>DÃ©duplication]
        State[State Management<br/>Sessions Utilisateurs]
    end

    %% === SPARK CORE ===
    subgraph SparkCore["Apache Spark Cluster - Traitement DistribuÃ©"]
        SparkSession[Spark Session Manager]
        
        subgraph SparkStreaming["Spark Streaming - Temps RÃ©el"]
            StreamProc[Stream Processing]
            WindowOps[Window Operations]
            RealTimeAgg[AgrÃ©gations Temps RÃ©el]
        end
        
        subgraph SparkBatch["Spark Batch - Traitement Lourd"]
            DataFrames[DataFrames API]
            SparkSQL[Spark SQL]
            MLlib[MLlib Machine Learning]
            GraphX[GraphX Processing]
        end
        
        SparkOptimizer[Query Optimizer]
    end

    %% === DATA LAKE ===
    subgraph DataLake["Data Lake - Stockage Persistant"]
        Bronze[Bronze Layer<br/>Data Brute]
        Silver[Silver Layer<br/>Data NettoyÃ©e]
        Gold[Gold Layer<br/>Data AgrÃ©gÃ©e]
    end

    %% === SERVING LAYER ===
    subgraph ServingLayer["Serving Layer - AccÃ¨s aux DonnÃ©es"]
        PostgreSQL[PostgreSQL<br/>DonnÃ©es OpÃ©rationnelles]
        APIServe[API Serving Layer<br/>REST/GraphQL]
        Analytics[Analytics Dashboard]
    end

    %% === CONNECTIONS ===
    Orchestration --> DataCollection
    
    DataCollection --> Streams
    DataCollection --> Cache
    
    Streams --> SparkStreaming
    Cache --> SparkBatch
    Dedup --> SparkStreaming
    State --> SparkStreaming
    
    SparkStreaming --> Bronze
    SparkBatch --> Silver
    SparkBatch --> Gold
    
    Bronze --> SparkBatch
    Silver --> SparkBatch
    Gold --> ServingLayer
    
    SparkStreaming --> APIServe
    Gold --> PostgreSQL
    Gold --> Analytics
```


# Utilisation
```bash
# DÃ©marrer Spark
docker-compose --profile spark up -d

# Supprimer Spark
docker-compose --profile spark down -v --rmi all
```



# VÃ©rifier que tout fonctionne
 Ouvrir http://localhost:8080

```bash
# Tester un job
docker-compose exec spark-submit /opt/scripts/submit-job.sh /opt/scripts/wordcount.py

```


# Utilisation
```bash
# PrÃ©alable
# si necesseaire
docker-compose exec spark-submit pip install python-dotenv

# ExÃ©cuter le script spark_importer.py
docker-compose exec spark-submit /opt/scripts/submit-job.sh /opt/scripts/spark_importer.py


# Note: pour copier le script et les donnÃ©es dans le conteneur Spark (pas necessaire car prÃ©sent dans scripts)
docker cp spark_importer.py imo-ops-spark-submit-1:/opt/scripts/
docker cp annonces_agregees.json imo-ops-spark-submit-1:/opt/data/
docker cp config_aggreges.json imo-ops-spark-submit-1:/opt/scripts/




```

![](img/spark00.png)
![](img/spark01.png)
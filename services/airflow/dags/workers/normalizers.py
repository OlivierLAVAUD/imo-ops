from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from shared.redis_client import redis_client
import json
import logging

def normaliser_donnees_api():
    """Normalisation des donnÃ©es API - VERSION REDIS ACTIF"""
    print("ðŸ”„ Normalisation des donnÃ©es API...")
    
    donnees_normalisees = []
    max_records = 500  # Limite de sÃ©curitÃ©
    
    try:
        queue_length = redis_client.get_queue_length('queue_api')
        print(f"ðŸ”´ Queue API: {queue_length} Ã©lÃ©ments")
        
        records_processed = 0
        while redis_client.get_queue_length('queue_api') > 0 and records_processed < max_records:
            donnee = redis_client.pop_from_queue('queue_api')
            if donnee:
                # Conversion et validation des donnÃ©es
                if isinstance(donnee, str):
                    try:
                        donnee = json.loads(donnee)
                    except json.JSONDecodeError:
                        print(f"âš ï¸ DonnÃ©e API non JSON ignorÃ©e: {donnee[:100]}...")
                        continue
                
                # Normalisation standardisÃ©e
                donnee_normalisee = {
                    'source': 'api',
                    'data': donnee,
                    'normalized': True,
                    'normalized_timestamp': datetime.now().isoformat(),
                    'format': 'standard_v1',
                    'processing_step': 'normalization'
                }
                
                donnees_normalisees.append(donnee_normalisee)
                redis_client.push_to_queue('queue_normalized', json.dumps(donnee_normalisee))
                records_processed += 1
        
        print(f"âœ… {len(donnees_normalisees)} donnÃ©es API normalisÃ©es")
        return f"API_NORMALIZED_{len(donnees_normalisees)}"
        
    except Exception as e:
        print(f"âŒ Erreur normalisation API: {e}")
        return f"API_ERROR_{str(e)}"

def normaliser_donnees_fichiers():
    """Normalisation des donnÃ©es fichiers - VERSION REDIS ACTIF"""
    print("ðŸ”„ Normalisation des donnÃ©es fichiers...")
    
    donnees_normalisees = []
    max_records = 500
    
    try:
        queue_length = redis_client.get_queue_length('queue_file')
        print(f"ðŸ”´ Queue Fichiers: {queue_length} Ã©lÃ©ments")
        
        records_processed = 0
        while redis_client.get_queue_length('queue_file') > 0 and records_processed < max_records:
            donnee = redis_client.pop_from_queue('queue_file')
            if donnee:
                if isinstance(donnee, str):
                    try:
                        donnee = json.loads(donnee)
                    except json.JSONDecodeError:
                        print(f"âš ï¸ DonnÃ©e fichier non JSON ignorÃ©e: {donnee[:100]}...")
                        continue
                
                donnee_normalisee = {
                    'source': 'file',
                    'data': donnee,
                    'normalized': True,
                    'normalized_timestamp': datetime.now().isoformat(),
                    'format': 'standard_v1',
                    'processing_step': 'normalization'
                }
                
                donnees_normalisees.append(donnee_normalisee)
                redis_client.push_to_queue('queue_normalized', json.dumps(donnee_normalisee))
                records_processed += 1
        
        print(f"âœ… {len(donnees_normalisees)} donnÃ©es fichiers normalisÃ©es")
        return f"FILES_NORMALIZED_{len(donnees_normalisees)}"
        
    except Exception as e:
        print(f"âŒ Erreur normalisation fichiers: {e}")
        return f"FILES_ERROR_{str(e)}"

def normaliser_donnees_web():
    """Normalisation des donnÃ©es web scraping - VERSION REDIS ACTIF"""
    print("ðŸ”„ Normalisation des donnÃ©es web...")
    
    donnees_normalisees = []
    max_records = 500
    
    try:
        queue_length = redis_client.get_queue_length('queue_web')
        print(f"ðŸ”´ Queue Web: {queue_length} Ã©lÃ©ments")
        
        records_processed = 0
        while redis_client.get_queue_length('queue_web') > 0 and records_processed < max_records:
            donnee = redis_client.pop_from_queue('queue_web')
            if donnee:
                if isinstance(donnee, str):
                    try:
                        donnee = json.loads(donnee)
                    except json.JSONDecodeError:
                        print(f"âš ï¸ DonnÃ©e web non JSON ignorÃ©e: {donnee[:100]}...")
                        continue
                
                donnee_normalisee = {
                    'source': 'web',
                    'data': donnee,
                    'normalized': True,
                    'normalized_timestamp': datetime.now().isoformat(),
                    'format': 'standard_v1',
                    'processing_step': 'normalization'
                }
                
                donnees_normalisees.append(donnee_normalisee)
                redis_client.push_to_queue('queue_normalized', json.dumps(donnee_normalisee))
                records_processed += 1
        
        print(f"âœ… {len(donnees_normalisees)} donnÃ©es web normalisÃ©es")
        return f"WEB_NORMALIZED_{len(donnees_normalisees)}"
        
    except Exception as e:
        print(f"âŒ Erreur normalisation web: {e}")
        return f"WEB_ERROR_{str(e)}"

def normaliser_donnees_db():
    """Normalisation des donnÃ©es bases de donnÃ©es - VERSION REDIS ACTIF"""
    print("ðŸ”„ Normalisation des donnÃ©es BD...")
    
    donnees_normalisees = []
    max_records = 500
    
    try:
        queue_length = redis_client.get_queue_length('queue_db')
        print(f"ðŸ”´ Queue BD: {queue_length} Ã©lÃ©ments")
        
        records_processed = 0
        while redis_client.get_queue_length('queue_db') > 0 and records_processed < max_records:
            donnee = redis_client.pop_from_queue('queue_db')
            if donnee:
                if isinstance(donnee, str):
                    try:
                        donnee = json.loads(donnee)
                    except json.JSONDecodeError:
                        print(f"âš ï¸ DonnÃ©e BD non JSON ignorÃ©e: {donnee[:100]}...")
                        continue
                
                donnee_normalisee = {
                    'source': 'database',
                    'data': donnee,
                    'normalized': True,
                    'normalized_timestamp': datetime.now().isoformat(),
                    'format': 'standard_v1',
                    'processing_step': 'normalization'
                }
                
                donnees_normalisees.append(donnee_normalisee)
                redis_client.push_to_queue('queue_normalized', json.dumps(donnee_normalisee))
                records_processed += 1
        
        print(f"âœ… {len(donnees_normalisees)} donnÃ©es BD normalisÃ©es")
        return f"DB_NORMALIZED_{len(donnees_normalisees)}"
        
    except Exception as e:
        print(f"âŒ Erreur normalisation BD: {e}")
        return f"DB_ERROR_{str(e)}"

def verifier_queues_normalisation():
    """VÃ©rification des queues avant/aprÃ¨s normalisation"""
    print("ðŸ” Ã‰TAT DES QUEUES DE NORMALISATION:")
    
    try:
        queues = {
            'API': 'queue_api',
            'Fichiers': 'queue_file',
            'Web': 'queue_web',
            'BD': 'queue_db',
            'NormalisÃ©es': 'queue_normalized'
        }
        
        for nom, queue in queues.items():
            length = redis_client.get_queue_length(queue)
            status = "âœ… VIDE" if length == 0 else f"ðŸ”´ {length}"
            print(f"   {nom} ({queue}): {status}")
            
            # AperÃ§u pour les queues d'entrÃ©e non vides
            if queue != 'queue_normalized' and length > 0:
                try:
                    preview = redis_client.peek_queue(queue)
                    if preview:
                        preview_str = str(preview)[:80] + "..." if len(str(preview)) > 80 else str(preview)
                        print(f"      ðŸ‘€ AperÃ§u: {preview_str}")
                except:
                    pass
        
        return "QUEUES_CHECKED"
        
    except Exception as e:
        print(f"âŒ Erreur vÃ©rification queues: {e}")
        return f"QUEUES_ERROR_{str(e)}"

with DAG(
    'imo_t_workers_normalization',
    default_args={
        'owner': 'airflow', 
        'retries': 2,
        'retry_delay': timedelta(minutes=3)
    },
    description='Workers de normalisation des donnÃ©es - REDIS ACTIF',
    schedule_interval=timedelta(minutes=15),  # Toutes les 15 minutes
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['workers', 'normalization', 'redis'],
) as dag:

    check_queues = PythonOperator(
        task_id='verifier_queues_normalisation',
        python_callable=verifier_queues_normalisation,
    )

    norm_api = PythonOperator(
        task_id='normaliser_donnees_api',
        python_callable=normaliser_donnees_api,
    )

    norm_files = PythonOperator(
        task_id='normaliser_donnees_fichiers',
        python_callable=normaliser_donnees_fichiers,
    )

    norm_web = PythonOperator(
        task_id='normaliser_donnees_web',
        python_callable=normaliser_donnees_web,
    )

    norm_db = PythonOperator(
        task_id='normaliser_donnees_db',
        python_callable=normaliser_donnees_db,
    )

    # Workflow: vÃ©rification â†’ normalisation en parallÃ¨le
    check_queues >> [norm_api, norm_files, norm_web, norm_db]
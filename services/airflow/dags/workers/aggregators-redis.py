from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from shared.redis_client import redis_client
import json
import logging

def agreger_donnees():
    """Agr√©gation de toutes les donn√©es normalis√©es - VERSION REDIS ACTIF"""
    print("üìä D√©marrage de l'agr√©gation des donn√©es depuis Redis...")
    
    donnees_aggregees = {
        'metadata': {
            'aggregation_timestamp': datetime.now().isoformat(),
            'total_records': 0,
            'sources': [],
            'aggregation_type': 'redis_auto',
            'environment': 'imo-ops',
            'redis_queue': 'queue_normalized'
        }
    }
    
    # COLLECTE R√âELLE DEPUIS REDIS
    try:
        queue_length = redis_client.get_queue_length('queue_normalized')
        print(f"üî¥ Redis - √âl√©ments dans queue_normalized: {queue_length}")
        
        records_processed = 0
        max_records = 1000  # Limite pour √©viter les boucles infinies
        donnees_collectees = []
        
        while redis_client.get_queue_length('queue_normalized') > 0 and records_processed < max_records:
            donnee = redis_client.pop_from_queue('queue_normalized')
            if donnee:
                # Conversion des donn√©es si n√©cessaire
                if isinstance(donnee, str):
                    try:
                        donnee = json.loads(donnee)
                    except json.JSONDecodeError:
                        print(f"‚ö†Ô∏è Donn√©e non JSON ignor√©e: {donnee[:100]}...")
                        continue
                
                donnees_collectees.append(donnee)
                records_processed += 1
                
                # Extraire la source
                source = donnee.get('source', 'unknown')
                if source not in donnees_aggregees['metadata']['sources']:
                    donnees_aggregees['metadata']['sources'].append(source)
        
        donnees_aggregees['metadata']['total_records'] = len(donnees_collectees)
        donnees_aggregees['metadata']['redis_initial_count'] = queue_length
        donnees_aggregees['metadata']['records_processed'] = records_processed
        
        print(f"‚úÖ {len(donnees_collectees)} donn√©es agr√©g√©es depuis Redis")
        print(f"üåê Sources d√©tect√©es: {donnees_aggregees['metadata']['sources']}")
        
        # V√©rifier s'il reste des donn√©es
        remaining = redis_client.get_queue_length('queue_normalized')
        if remaining > 0:
            print(f"‚ö†Ô∏è  Il reste {remaining} √©l√©ments dans la queue - limite atteinte")
        
    except Exception as e:
        print(f"‚ùå Erreur collecte Redis: {e}")
        donnees_aggregees['metadata']['error'] = str(e)
        donnees_aggregees['metadata']['fallback_mode'] = True
    
    # Sauvegarde dans PostgreSQL
    sauvegarder_postgresql(donnees_aggregees)
    
    return f"DATA_AGGREGATED_{donnees_aggregees['metadata']['total_records']}"

def sauvegarder_postgresql(donnees_aggregees):
    """Sauvegarde des donn√©es agr√©g√©es dans PostgreSQL"""
    try:
        hook = PostgresHook(postgres_conn_id='imo_db')
        
        # V√©rifier si la table existe
        table_exists = hook.get_first("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public'
                AND table_name = 'donnees_aggregees'
            );
        """)
        
        if not table_exists or not table_exists[0]:
            print("‚ùå Table 'donnees_aggregees' n'existe pas dans imo_db")
            raise Exception("Table donnees_aggregees non trouv√©e")
        
        # Insertion des donn√©es
        insert_sql = """
        INSERT INTO donnees_aggregees (metadata, created_at)
        VALUES (%s, %s)
        RETURNING id;
        """
        
        result = hook.get_first(insert_sql, parameters=(
            json.dumps(donnees_aggregees['metadata']),
            datetime.now()
        ))
        
        if result:
            agg_id = result[0]
            print(f"üíæ Donn√©es sauvegard√©es dans PostgreSQL (ID: {agg_id})")
            
            # Log d√©taill√©
            print("üìã R√âSUM√â DE L'AGR√âGATION:")
            metadata = donnees_aggregees['metadata']
            print(f"   ‚Ä¢ Enregistrements: {metadata['total_records']}")
            print(f"   ‚Ä¢ Sources: {metadata['sources']}")
            print(f"   ‚Ä¢ Queue initiale: {metadata.get('redis_initial_count', 'N/A')}")
            print(f"   ‚Ä¢ Trait√©s: {metadata.get('records_processed', 'N/A')}")
            print(f"   ‚Ä¢ Type: {metadata['aggregation_type']}")
            
        else:
            print("‚ö†Ô∏è Aucun ID retourn√© lors de l'insertion")
        
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde PostgreSQL: {e}")
        logging.error(f"Erreur sauvegarde PostgreSQL: {e}")

def verifier_etat_redis():
    """V√©rification compl√®te de l'√©tat Redis"""
    try:
        print("üî¥ √âTAT COMPLET REDIS:")
        
        queues = {
            'API': 'queue_api',
            'Fichiers': 'queue_file', 
            'Web': 'queue_web',
            'BD': 'queue_db',
            'Normalis√©es': 'queue_normalized'
        }
        
        status_global = {}
        
        for nom, queue in queues.items():
            length = redis_client.get_queue_length(queue)
            status_global[queue] = length
            
            # √âtat d√©taill√©
            if length == 0:
                print(f"   ‚úÖ {nom} ({queue}): VIDE")
            elif length < 10:
                print(f"   ‚ö†Ô∏è  {nom} ({queue}): {length} √©l√©ments")
            else:
                print(f"   üî• {nom} ({queue}): {length} √©l√©ments")
        
        # R√©sum√© global
        total_elements = sum(status_global.values())
        queues_non_vides = [q for q, l in status_global.items() if l > 0]
        
        print(f"üìä R√âSUM√â REDIS:")
        print(f"   ‚Ä¢ Total √©l√©ments: {total_elements}")
        print(f"   ‚Ä¢ Queues non vides: {len(queues_non_vides)}")
        print(f"   ‚Ä¢ Queues: {queues_non_vides}")
        
        return f"REDIS_CHECK_{total_elements}_ITEMS"
        
    except Exception as e:
        print(f"‚ùå Erreur v√©rification Redis: {e}")
        return f"REDIS_ERROR_{str(e)}"

def verifier_agregation():
    """V√©rification de l'agr√©gation r√©cente"""
    try:
        hook = PostgresHook(postgres_conn_id='imo_db')
        
        # Derni√®re agr√©gation
        derniere_agg = hook.get_first("""
            SELECT 
                id,
                metadata->>'total_records' as total_records,
                metadata->>'sources' as sources,
                metadata->>'redis_initial_count' as redis_count,
                created_at
            FROM donnees_aggregees 
            ORDER BY created_at DESC 
            LIMIT 1;
        """)
        
        if derniere_agg:
            agg_id, total_records, sources, redis_count, created_at = derniere_agg
            print("‚úÖ DERNI√àRE AGR√âGATION V√âRIFI√âE:")
            print(f"   ‚Ä¢ ID: {agg_id}")
            print(f"   ‚Ä¢ Enregistrements: {total_records}")
            print(f"   ‚Ä¢ Redis initial: {redis_count}")
            print(f"   ‚Ä¢ Sources: {sources}")
            print(f"   ‚Ä¢ Date: {created_at}")
        else:
            print("‚ÑπÔ∏è Aucune agr√©gation trouv√©e dans la base")
            
        return "VERIFICATION_COMPLETED"
        
    except Exception as e:
        print(f"‚ùå Erreur v√©rification: {e}")
        return f"VERIFICATION_ERROR_{str(e)}"

def nettoyer_agregations_anciennes():
    """Nettoyage des agr√©gations anciennes"""
    try:
        hook = PostgresHook(postgres_conn_id='imo_db')
        
        # Compter avant nettoyage
        count_avant = hook.get_first("SELECT COUNT(*) FROM donnees_aggregees;")
        
        # Supprimer les agr√©gations de plus de 30 jours
        delete_sql = """
        DELETE FROM donnees_aggregees 
        WHERE created_at < NOW() - INTERVAL '30 days';
        """
        
        hook.run(delete_sql)
        
        # Compter apr√®s nettoyage
        count_apres = hook.get_first("SELECT COUNT(*) FROM donnees_aggregees;")
        supprimes = (count_avant[0] if count_avant else 0) - (count_apres[0] if count_apres else 0)
        
        print(f"üßπ Nettoyage des agr√©gations anciennes:")
        print(f"   ‚Ä¢ Agr√©gations supprim√©es: {supprimes}")
        print(f"   ‚Ä¢ Agr√©gations restantes: {count_apres[0] if count_apres else 0}")
        
        return "CLEANUP_COMPLETED"
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur nettoyage: {e}")
        return f"CLEANUP_ERROR_{str(e)}"


with DAG(
    'imo_t_workers_aggregation_redis',
    default_args={
        'owner': 'airflow', 
        'retries': 2,
        'retry_delay': timedelta(minutes=5)  
    },
    description='Workers d agr√©gation des donn√©es - REDIS ACTIF',
    schedule_interval=timedelta(hours=1),  
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['workers', 'aggregation', 'imo_db', 'redis'],
) as dag:

    check_redis = PythonOperator(
        task_id='verifier_etat_redis',
        python_callable=verifier_etat_redis,
    )

    aggregate_data = PythonOperator(
        task_id='agreger_donnees',
        python_callable=agreger_donnees,
    )

    verify_aggregation = PythonOperator(
        task_id='verifier_agregation',
        python_callable=verifier_agregation,
    )

    cleanup_old = PythonOperator(
        task_id='nettoyer_agregations_anciennes',
        python_callable=nettoyer_agregations_anciennes,
    )

    # Workflow: v√©rification Redis ‚Üí agr√©gation ‚Üí v√©rification ‚Üí nettoyage
    check_redis >> aggregate_data >> verify_aggregation >> cleanup_old
# Web Scraping to Json files (c1-scrap)

Ce projet est un scraper automatis√© d√©velopp√© en Python pour extraire des donn√©es immobili√®res depuis le site IAD France. 

Utilisant Playwright pour le navigateur headless, il permet de collecter des informations d√©taill√©es sur les annonces immobili√®res incluant les caract√©ristiques du bien, les prix, les photos, les performances √©nerg√©tiques (DPE/GES), les informations de copropri√©t√© et les coordonn√©es des conseillers.

 Le scraper int√®gre une gestion des cookies , une extraction avanc√©e des m√©dias (photos, vid√©os, visites virtuelles), et supporte la pagination pour collecter des donn√©es √† grande √©chelle.
 
 Configuration via fichier JSON, export des r√©sultats structur√©s et param√©trage flexible font de cet outil une architecture de solution compl√®te pour l'analyse du march√© immobilier fran√ßais √† partir du web.

## üõ† Stack Technologique
### Langage & Environnement

    - Python 3.7+
    - Asyncio - Pour le traitement concurrent et les op√©rations I/O non-bloquantes

### Web Scraping & Automatisation
    - Playwright - Navigation headless moderne avec support multi-navigateurs
    - Async API - Version asynchrone pour des performances optimales

### Traitement des Donn√©es

    - JSON - Configuration et export des donn√©es
    - Regex - Extraction et nettoyage des textes
    - Typing - Annotations de types pour la maintenabilit√©

### Architecture & Conception

    - Classes Sp√©cialis√©es :
        - CookieManager - Gestion intelligente des consentements
        - DataExtractor - Extraction structur√©e des donn√©es
        - MediaExtractor - Traitement des m√©dias et photos
        - PerformanceEnergetiqueExtractor - Analyse DPE/GES

### S√©lecteurs & Parsing

    - CSS Selectors - Localisation des √©l√©ments principaux
    - XPath - S√©lecteurs avanc√©s pour cas complexes
    - Text Processing - Nettoyage et validation des donn√©es

### Gestion de Configuration

    - Fichiers JSON - Configuration flexible du scraping
    - Variables d'environnement - Param√©trage d√©ploiement
    - Arguments CLI - Interface en ligne de commande
    - Gestion d'erreurs robuste - Continuit√© de service
    - Pagination automatique - Collecte multi-pages
    - D√©lais configurables - Respect des politiques sites
    - Export structur√© - Donn√©es pr√™tes pour analyse

# Configuration

    - config.json: fichier r√©f√©rencant les proprietes d'acessibilit√© du site web, d√©sirant √™tre collect√©es
    - config-playwright.json: d√©termine la structure de sortie du json en sortie:

# Usage

## with sources
```bash
# scrape avec les valeurs par defaut
uv run iad_scraper.py

# ‚Üí Scrape 20 biens √† Bordeaux sur 5 pages maximum
uv run iad_scraper.py --localisation "Bordeaux" --max-biens 20 --max-pages 5
```

## with dockerfile

```bash
# 1. Demarrer le service de scraping (le conteneur reste d√©marr√© et actif)
docker-compose up -d

# 2. Lancer la requete de scraping
docker exec iad-scraper python iad_scraper.py --localisation "Tours" --max-bien 1
docker exec iad-scraper python iad_scraper.py --localisation "Montpellier" --transaction louer --max-biens 1
docker exec iad-scraper python iad_scraper.py --localisation "Lyon" --transaction louer --max-biens 1
docker exec iad-scraper python iad_scraper.py --localisation "Montpellier" --transaction acheter --bien prestige --max-biens 1

#Note:
#usage: iad_scraper.py [-h] [--localisation LOCALISATION]
                      [--transaction {acheter,louer,vendre}]
                      [--bien {ancien,neuf,prestige,international,terrain,entreprises_commerces,immeuble}]
                      [--max-biens MAX_BIENS] [--max-pages MAX_PAGES]


# 3. Voir les fichiers dans le conteneur
docker exec iad-scraper ls -la /app/results/

# 4. Copier tout le r√©pertoire de resultats produits
docker cp iad-scraper:/app/results/ ./downloads/

# 5. Copier un fichier sp√©cifique du conteneur vers votre machine
docker cp iad-scraper:/app/results/mon_fichier.json ./downloads/

# 6. Arr√™ter & supprimer le conteneur, images, .. associ√©es
docker-compose --profile scraping down -v --rmi all

```

docker exec -it iad-scraper /bin/bash
python iad_scraper.py --localisation "Tours" --max-bien 1


docker exec -it iad-scraper python iad_scraper.py --localisation "Tours" --max-bien 1
![imo_ops](img/image.png)

# IMO-Ops: Data Plateform IMO-Ops pour un projet en intelligence artificielle d√©di√© au march√© Immobilier


## Contexte & objectifs:
- Une application IA, par exemple un chatbot LLM RAG a besoin d'acc√®s √† des donn√©es en temps r√©el aupr√®s de diff√©rentes sources du march√© immobilier. L‚Äôobjectif est donc de produire une Data Plateform d√©di√© au march√© immobilier


L‚Äôarchitecture ( √©cosyst√®me/stack technologique)
- Orchestration de pipelines avec messaging, parall√©lisation & mise √† l‚Äô√©chelle (Apache Airflow+Redis - Dags - Logs)
- Scraping: Playwright avec fichiers de configuration
- Manipulation et transformation des donn√©es: Pandas, Regex, SQLAlchemy, Requests
- Bases de donn√©es: Sql (PostgreSQL-pgadmin) & NoSQL (MongoDB)
- API: FastAPI
- Rapports & consultation des donn√©es (Prometheus- Grafana, Plotly-Dashboards, Gradio)
- Conteneurisation: Docker

Les processus:
- batchs/pipelines de collecte √† partir de diff√©rentes sources: scraping web, fast API, fichiers csv, bases de donn√©es
postgres, fichiers csv
- batchs/pipelines de normalisation apr√®s l‚Äôextraction des donn√©es
- batch/pipeline d‚Äôagr√©gation des donn√©es en provenance des diff√©rentes sources
- batch/pipelines de stockage en base de donn√©es SQL & NoSQL
- batch/pipeline d‚Äôinterrogation des donn√©es
- batch/pipeline de mise √† disposition des donn√©es

Les sources de donn√©es:
site web: IAD-Immobilier (https://www.iadfrance.fr/): annonces achats, ventes, location sur crit√®res multiples: 

# Architecture
## Architecture Airflow-Redis-Postgres-Mongodb pour MLOps


```mermaid
flowchart TB

  %% === ORCHESTRATEUR PRINCIPAL ===
  subgraph MainOrchestrator["ORCHESTRATEUR PRINCIPAL - main_orchestrator.py"]
    MainOrch[Main Orchestrator - Apache Airflow]
  end

  %% === PIPELINES ===
  subgraph Pipelines["Pipelines de Collecte"]
    PipeAPI[Pipeline API]
    PipeFile[Pipeline Fichier]
    PipeWeb[Pipeline Web Scraping]
    PipeDB[Pipeline Base de Donn√©es SQL - No SQL]
  end

  %% === REDIS QUEUES ===
  subgraph RedisQ["Redis Queues par Source"]
    QueueAPI[Redis Queue - API]
    QueueFile[Redis Queue - Fichier]
    QueueWeb[Redis Queue - Scraping]
    QueueDB[Redis Queue - BD]
  end

  %% === MESSAGE BROKER ===
  subgraph Broker["Message Broker - Redis"]
    BrokerCore[Gestion des files - Communication inter-pipelines]
  end

  %% === WORKERS DE NORMALISATION ===
  subgraph Normalizers["Workers de Normalisation"]
    WorkerAPI[Worker API]
    WorkerFile[Worker Fichiers]
    WorkerWeb[Worker Web Scraping]
    WorkerDB[Worker BD]
  end

  %% === WORKERS D'AGREGATION ===
  subgraph Aggregators["Workers d'Agr√©gation"]
    AggAPI[Aggregator API + Fichiers + Scraping web + BD]

  end

  %% === BASES DE DONN√âES ===
  subgraph Databases["Bases de Donn√©es/ Big Data"]
    DBAPI[(BD Normalis√©e & Agr√©g√©e - PostgreSQL - MongoDB)]
  end



  %% === LIENS ===
  MainOrch --> PipeAPI
  MainOrch --> PipeFile
  MainOrch --> PipeWeb
  MainOrch --> PipeDB

  PipeAPI --> QueueAPI
  PipeFile --> QueueFile
  PipeWeb --> QueueWeb
  PipeDB --> QueueDB

  QueueAPI --> BrokerCore
  QueueFile --> BrokerCore
  QueueWeb --> BrokerCore
  QueueDB --> BrokerCore

  BrokerCore --> WorkerAPI
  BrokerCore --> WorkerFile
  BrokerCore --> WorkerWeb
  BrokerCore --> WorkerDB

  WorkerAPI --> AggAPI
  WorkerFile --> AggAPI
  WorkerWeb --> AggAPI
  WorkerDB --> AggAPI

  AggAPI --> DBAPI
  
```
## Architecture Airflow-Redis-Spark-Postgres pour MLOps
```mermaid
flowchart TB

  %% === ORCHESTRATEUR PRINCIPAL ===
  subgraph MainOrchestrator["ORCHESTRATEUR PRINCIPAL - main_orchestrator.py"]
    MainOrch[Main Orchestrator - Apache Airflow]
  end

  %% === PIPELINES ===
  subgraph Pipelines["Pipelines de Collecte"]
    PipeAPI[Pipeline API]
    PipeFile[Pipeline Fichier]
    PipeWeb[Pipeline Web Scraping]
    PipeDB[Pipeline Base de Donn√©es SQL - No SQL]
  end

  %% === REDIS QUEUES ===
  subgraph RedisQ["Redis Queues par Source"]
    QueueAPI[Redis Queue - API]
    QueueFile[Redis Queue - Fichier]
    QueueWeb[Redis Queue - Scraping]
    QueueDB[Redis Queue - BD]
  end

  %% === MESSAGE BROKER ===
  subgraph Broker["Message Broker - Redis"]
    BrokerCore[Gestion des files - Communication inter-pipelines]
  end

  %% === WORKERS DE NORMALISATION ===
  subgraph Normalizers["Workers de Normalisation"]
    WorkerAPI[Worker API]
    WorkerFile[Worker Fichiers]
    WorkerWeb[Worker Web Scraping]
    WorkerDB[Worker BD]
  end

  %% === SPARK CORE ===
  subgraph SparkCore["Apache Spark Cluster"]
    SparkSession[Spark Session]
    SparkSQL[Spark SQL - DataFrames]
    SparkStreaming[Spark Streaming]
    SparkML[MLlib - Machine Learning]
  end

  %% === WORKERS D'AGREGATION ===
  subgraph Aggregators["Workers d'Agr√©gation Spark"]
    AggAPI[Spark Aggregator - Traitement Distribu√©]
  end

  %% === STOCKAGE SPARK ===
  subgraph SparkStorage["Stockage Spark & Data Lake"]
    DataLake[(Data Lake - Parquet/Delta)]
    SparkWarehouse[(Spark Warehouse)]
  end

  %% === BASES FINALES ===
  subgraph FinalDBs["Bases Finales Optimis√©es"]
    AnalyticsDB[(PostgreSQL - Analytics)]
    ServingDB[(API Serving Layer)]
  end

  %% === LIENS ===
  MainOrch --> PipeAPI
  MainOrch --> PipeFile
  MainOrch --> PipeWeb
  MainOrch --> PipeDB

  PipeAPI --> QueueAPI
  PipeFile --> QueueFile
  PipeWeb --> QueueWeb
  PipeDB --> QueueDB

  QueueAPI --> BrokerCore
  QueueFile --> BrokerCore
  QueueWeb --> BrokerCore
  QueueDB --> BrokerCore

  BrokerCore --> WorkerAPI
  BrokerCore --> WorkerFile
  BrokerCore --> WorkerWeb
  BrokerCore --> WorkerDB

  WorkerAPI --> SparkSession
  WorkerFile --> SparkSession
  WorkerWeb --> SparkSession
  WorkerDB --> SparkSession

  SparkSession --> AggAPI
  AggAPI --> SparkSQL
  AggAPI --> SparkStreaming
  AggAPI --> SparkML
  
  SparkSQL --> DataLake
  SparkStreaming --> DataLake
  SparkML --> DataLake
  
  DataLake --> SparkWarehouse
  SparkWarehouse --> AnalyticsDB
  SparkWarehouse --> ServingDB
```


# Prerequisite

  - uv (https://docs.astral.sh/uv/guides/install-python/)
  - Apache Airflow (https://airflow.apache.org/)   
  - Postgres SQL (https://www.postgresql.org/download/windows/)
  - Mongodb (https://www.mongodb.com/docs/manual/installation/)
  - Redis (https://redis.io/)
  - Plawright (https://playwright.dev/)

# Installation
-  Clone the repo and access to files
```bash
git clone https://github.com/OlivierLAVAUD/imo-ops.git

cd imo-ops
```

## install with docker

- Launch Docker-Desktop before
- ..., then launch docker services
```bash
# for all services
docker-compose up -d

# for Airflow profile services
docker-compose --profile airflow up -d

# for noSQL Database: MongoDB profile services
docker-compose --profile mongodb up -d

# for Gradio app with gradio profile services
docker-compose --profile gradio up -d

# for Spark
docker-compose --profile spark up -d
```
- check
```bash
# V√©rifier que tout fonctionne
docker-compose ps
```
Then acess to the applications from Docker Desktop with https://localhost:<ports>



- Spark 
```bash
# Tester un job
docker-compose exec spark-submit /opt/scripts/submit-job.sh /opt/scripts/wordcount.py

# Ou tester spark-shell interactif
docker-compose exec spark-submit /opt/spark/bin/spark-shell

# Red√©marrer compl√®tement
docker-compose --profile spark down -v --rmi all
docker-compose --profile spark up -d

```
üåê URLs d'acc√®s :

    - Spark Master UI : http://localhost:8085 
    - Spark History Server : http://localhost:18080
    - Airflow : http://localhost:8080

![spark](img/spark-3.png)
![spark](img/spark-5.png)
![spark](img/spark-4.png)


## Comp√©tences
###  Comp√©tence C1: Automatisation de l'extraction de donn√©es (c1_scrap Service:)

```bash
cd c1_scrap
# see the README.md file
```
### Comp√©tences C2: D√©veloppement de requ√™tes SQL(c2_sql Service)
```bash
cd c2_sql
# see the README.md file
```
### Comp√©tences C3:  Agr√©gation et pr√©paration des donn√©es (c3_aggr Service)
```bash
cd c3_aggr
# see the README.md file
```
### Comp√©tence C4 : Cr√©ation de Base de Donn√©es (c4_create_db Service)
```bash
cd c4_create_db
# see the README.md file
```

### Comp√©tence C5: D√©veloppement d‚Äôune API Rest (c5_api  Service)
```bash
cd c5_api
# see the README.md file
```


# Notes
## Docker

Install with docker

```bash
docker-compose --profile airflow up -d
```

Cleaning Docker
```bash

docker-compose --profile airflow down -v --rmi all

# Arr√™te et supprime les conteneurs, r√©seaux, volumes, images build√©es du projet courant
docker-compose down -v --rmi all

# Supprime le cache du build
docker builder prune
docker system prune -f

```

